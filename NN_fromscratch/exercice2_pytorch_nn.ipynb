{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = True, normalize = True, flatten = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = Variable(train_input), Variable(train_target)\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, mini_batch_size, eta=1e-1, \n",
    "                num_epochs=25, criterion=nn.MSELoss()):\n",
    "   \n",
    "    for e in range(0, num_epochs):\n",
    "        sum_loss = 0\n",
    "        # We do this with mini-batches\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in model.parameters():\n",
    "                p.data.sub_(eta * p.grad.data)\n",
    "        print(\"Loss\",e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 0.17489974480122328\n",
      "Loss 1 0.1647532070055604\n",
      "Loss 2 0.15682137198746204\n",
      "Loss 3 0.15737433265894651\n",
      "Loss 4 0.15803939662873745\n",
      "Loss 5 0.15815046429634094\n",
      "Loss 6 0.15859481040388346\n",
      "Loss 7 0.14998853206634521\n",
      "Loss 8 0.15034357644617558\n",
      "Loss 9 0.1533521907404065\n",
      "Loss 10 0.1434680251404643\n",
      "Loss 11 0.14030923042446375\n",
      "Loss 12 0.15024878084659576\n",
      "Loss 13 0.14576242584735155\n",
      "Loss 14 0.14165062177926302\n",
      "Loss 15 0.13995695859193802\n",
      "Loss 16 0.13601051550358534\n",
      "Loss 17 0.13578791078180075\n",
      "Loss 18 0.13432730454951525\n",
      "Loss 19 0.13639160990715027\n",
      "Loss 20 0.1342464480549097\n",
      "Loss 21 0.1295823808759451\n",
      "Loss 22 0.1309264963492751\n",
      "Loss 23 0.13282429054379463\n",
      "Loss 24 0.12577392999082804\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_input, train_target, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, test_input, test_target, mini_batch_size):\n",
    "    nb_error = 0.0\n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output_test = model(test_input.narrow(0, b, mini_batch_size))\n",
    "        i, predicted_classes = output_test.max(1)\n",
    "        #print(predicted_classes)\n",
    "        for c in range(0,mini_batch_size):\n",
    "            if test_target[b + c, predicted_classes[c]] <= 0:\n",
    "                nb_error +=1\n",
    "    return nb_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_target = Variable(test_input), Variable(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model,test_input,test_target,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 0.4810798093676567\n",
      "Loss 1 0.4284026026725769\n",
      "Loss 2 0.4097045436501503\n",
      "Loss 3 0.3931649923324585\n",
      "Loss 4 0.37750063091516495\n",
      "Loss 5 0.36229298263788223\n",
      "Loss 6 0.34756774455308914\n",
      "Loss 7 0.3334425762295723\n",
      "Loss 8 0.32016026601195335\n",
      "Loss 9 0.3076443709433079\n",
      "Loss 10 0.295965775847435\n",
      "Loss 11 0.2851091958582401\n",
      "Loss 12 0.27500100433826447\n",
      "Loss 13 0.2656019739806652\n",
      "Loss 14 0.2568773031234741\n",
      "Loss 15 0.24865758791565895\n",
      "Loss 16 0.24130580574274063\n",
      "Loss 17 0.23415334895253181\n",
      "Loss 18 0.24070346355438232\n",
      "Loss 19 0.28150344640016556\n",
      "Loss 20 0.23322124034166336\n",
      "Loss 21 0.21291149407625198\n",
      "Loss 22 0.20926125720143318\n",
      "Loss 23 0.2072732038795948\n",
      "Loss 24 0.23557151854038239\n",
      "test error Net 36.40% \n",
      "Loss 0 0.4673112630844116\n",
      "Loss 1 0.4227360785007477\n",
      "Loss 2 0.40531057864427567\n",
      "Loss 3 0.38897818326950073\n",
      "Loss 4 0.37300970405340195\n",
      "Loss 5 0.35730377584695816\n",
      "Loss 6 0.34221170097589493\n",
      "Loss 7 0.3279404640197754\n",
      "Loss 8 0.3146047778427601\n",
      "Loss 9 0.30220959708094597\n",
      "Loss 10 0.29080580174922943\n",
      "Loss 11 0.28039801493287086\n",
      "Loss 12 0.27095983549952507\n",
      "Loss 13 0.26226744800806046\n",
      "Loss 14 0.25440292432904243\n",
      "Loss 15 0.24719947576522827\n",
      "Loss 16 0.2406499795615673\n",
      "Loss 17 0.23456908017396927\n",
      "Loss 18 0.22913626581430435\n",
      "Loss 19 0.22483263537287712\n",
      "Loss 20 0.2720828726887703\n",
      "Loss 21 0.24829623103141785\n",
      "Loss 22 0.21420368179678917\n",
      "Loss 23 0.20776823908090591\n",
      "Loss 24 0.20321635529398918\n",
      "test error Net 21.00% \n",
      "Loss 0 0.49556348472833633\n",
      "Loss 1 0.43338000774383545\n",
      "Loss 2 0.41299837827682495\n",
      "Loss 3 0.3953999802470207\n",
      "Loss 4 0.3789990544319153\n",
      "Loss 5 0.36341897398233414\n",
      "Loss 6 0.34850600361824036\n",
      "Loss 7 0.33413059264421463\n",
      "Loss 8 0.32021644338965416\n",
      "Loss 9 0.3070000447332859\n",
      "Loss 10 0.29458197951316833\n",
      "Loss 11 0.2830367274582386\n",
      "Loss 12 0.27240150421857834\n",
      "Loss 13 0.26255981251597404\n",
      "Loss 14 0.25353821739554405\n",
      "Loss 15 0.24525005370378494\n",
      "Loss 16 0.23749367520213127\n",
      "Loss 17 0.23047054931521416\n",
      "Loss 18 0.22404658421874046\n",
      "Loss 19 0.23774925991892815\n",
      "Loss 20 0.2857188954949379\n",
      "Loss 21 0.21588652953505516\n",
      "Loss 22 0.2055441103875637\n",
      "Loss 23 0.19976890087127686\n",
      "Loss 24 0.19537007436156273\n",
      "test error Net 20.40% \n",
      "Loss 0 0.46623971313238144\n",
      "Loss 1 0.41896802932024\n",
      "Loss 2 0.3958842605352402\n",
      "Loss 3 0.3759707435965538\n",
      "Loss 4 0.3579629510641098\n",
      "Loss 5 0.34137435257434845\n",
      "Loss 6 0.32610441744327545\n",
      "Loss 7 0.3120703026652336\n",
      "Loss 8 0.2988453507423401\n",
      "Loss 9 0.28670572862029076\n",
      "Loss 10 0.2752872556447983\n",
      "Loss 11 0.2666347995400429\n",
      "Loss 12 0.2754329666495323\n",
      "Loss 13 0.3263159692287445\n",
      "Loss 14 0.24630289524793625\n",
      "Loss 15 0.2380150705575943\n",
      "Loss 16 0.2304852344095707\n",
      "Loss 17 0.2379583902657032\n",
      "Loss 18 0.27588052302598953\n",
      "Loss 19 0.22194619476795197\n",
      "Loss 20 0.21131102368235588\n",
      "Loss 21 0.2053767777979374\n",
      "Loss 22 0.20087998360395432\n",
      "Loss 23 0.19624725729227066\n",
      "Loss 24 0.21294260397553444\n",
      "test error Net 32.40% \n",
      "Loss 0 0.46308911591768265\n",
      "Loss 1 0.41911622881889343\n",
      "Loss 2 0.3989688977599144\n",
      "Loss 3 0.3807651177048683\n",
      "Loss 4 0.3637739345431328\n",
      "Loss 5 0.34776443988084793\n",
      "Loss 6 0.33248694986104965\n",
      "Loss 7 0.31798334047198296\n",
      "Loss 8 0.3043697848916054\n",
      "Loss 9 0.2916625998914242\n",
      "Loss 10 0.28000033646821976\n",
      "Loss 11 0.2693009525537491\n",
      "Loss 12 0.2595783583819866\n",
      "Loss 13 0.2506588213145733\n",
      "Loss 14 0.24264214932918549\n",
      "Loss 15 0.23532650619745255\n",
      "Loss 16 0.2286389209330082\n",
      "Loss 17 0.22403470799326897\n",
      "Loss 18 0.2624802589416504\n",
      "Loss 19 0.2727120593190193\n",
      "Loss 20 0.21050598844885826\n",
      "Loss 21 0.20429685339331627\n",
      "Loss 22 0.1994105912744999\n",
      "Loss 23 0.19507216289639473\n",
      "Loss 24 0.19112586602568626\n",
      "test error Net 19.90% \n",
      "Loss 0 0.4749355763196945\n",
      "Loss 1 0.4222665801644325\n",
      "Loss 2 0.3997994363307953\n",
      "Loss 3 0.37958284467458725\n",
      "Loss 4 0.36114737391471863\n",
      "Loss 5 0.3441762179136276\n",
      "Loss 6 0.32824231684207916\n",
      "Loss 7 0.31336070597171783\n",
      "Loss 8 0.29980579763650894\n",
      "Loss 9 0.2874327190220356\n",
      "Loss 10 0.27615654096007347\n",
      "Loss 11 0.26587069034576416\n",
      "Loss 12 0.25655055418610573\n",
      "Loss 13 0.24792547896504402\n",
      "Loss 14 0.24002759531140327\n",
      "Loss 15 0.23274102434515953\n",
      "Loss 16 0.22592688351869583\n",
      "Loss 17 0.21950193867087364\n",
      "Loss 18 0.21368791162967682\n",
      "Loss 19 0.2088206671178341\n",
      "Loss 20 0.22181838378310204\n",
      "Loss 21 0.26011716201901436\n",
      "Loss 22 0.20158815756440163\n",
      "Loss 23 0.19187861308455467\n",
      "Loss 24 0.18842970952391624\n",
      "test error Net 19.90% \n",
      "Loss 0 0.4932866096496582\n",
      "Loss 1 0.41229891777038574\n",
      "Loss 2 0.3904658481478691\n",
      "Loss 3 0.37067266553640366\n",
      "Loss 4 0.35219111293554306\n",
      "Loss 5 0.3350658267736435\n",
      "Loss 6 0.31911516562104225\n",
      "Loss 7 0.30440617725253105\n",
      "Loss 8 0.29116417467594147\n",
      "Loss 9 0.27919164299964905\n",
      "Loss 10 0.26845795661211014\n",
      "Loss 11 0.25882070511579514\n",
      "Loss 12 0.25014080479741096\n",
      "Loss 13 0.24228689074516296\n",
      "Loss 14 0.2352314107120037\n",
      "Loss 15 0.22877342253923416\n",
      "Loss 16 0.22317208349704742\n",
      "Loss 17 0.21779271960258484\n",
      "Loss 18 0.22128867730498314\n",
      "Loss 19 0.24201152101159096\n",
      "Loss 20 0.22545764967799187\n",
      "Loss 21 0.20224031060934067\n",
      "Loss 22 0.19869470223784447\n",
      "Loss 23 0.210373867303133\n",
      "Loss 24 0.21114631742238998\n",
      "test error Net 23.10% \n",
      "Loss 0 0.47927824407815933\n",
      "Loss 1 0.41092728823423386\n",
      "Loss 2 0.38696108013391495\n",
      "Loss 3 0.36619333922863007\n",
      "Loss 4 0.34789974987506866\n",
      "Loss 5 0.3315125033259392\n",
      "Loss 6 0.31657662615180016\n",
      "Loss 7 0.30283499881625175\n",
      "Loss 8 0.2906130328774452\n",
      "Loss 9 0.2793531082570553\n",
      "Loss 10 0.2694932743906975\n",
      "Loss 11 0.2616664879024029\n",
      "Loss 12 0.31333158165216446\n",
      "Loss 13 0.2973391264677048\n",
      "Loss 14 0.24635881930589676\n",
      "Loss 15 0.23897365853190422\n",
      "Loss 16 0.23121296986937523\n",
      "Loss 17 0.2663794979453087\n",
      "Loss 18 0.2524598464369774\n",
      "Loss 19 0.22195113822817802\n",
      "Loss 20 0.21227506920695305\n",
      "Loss 21 0.2077910490334034\n",
      "Loss 22 0.21342061460018158\n",
      "Loss 23 0.23906414210796356\n",
      "Loss 24 0.21536989137530327\n",
      "test error Net 22.00% \n",
      "Loss 0 0.46483607590198517\n",
      "Loss 1 0.4338543713092804\n",
      "Loss 2 0.417547382414341\n",
      "Loss 3 0.4030541181564331\n",
      "Loss 4 0.3895825147628784\n",
      "Loss 5 0.37660106271505356\n",
      "Loss 6 0.36377402395009995\n",
      "Loss 7 0.3509991988539696\n",
      "Loss 8 0.3382653221487999\n",
      "Loss 9 0.3256308138370514\n",
      "Loss 10 0.31321532651782036\n",
      "Loss 11 0.3013228811323643\n",
      "Loss 12 0.2900454141199589\n",
      "Loss 13 0.2794572450220585\n",
      "Loss 14 0.2695535682141781\n",
      "Loss 15 0.26037509739398956\n",
      "Loss 16 0.2518315017223358\n",
      "Loss 17 0.24394949153065681\n",
      "Loss 18 0.23666461929678917\n",
      "Loss 19 0.22989998757839203\n",
      "Loss 20 0.2237168401479721\n",
      "Loss 21 0.2185327112674713\n",
      "Loss 22 0.24257159605622292\n",
      "Loss 23 0.271441500633955\n",
      "Loss 24 0.20836099609732628\n",
      "test error Net 22.50% \n",
      "Loss 0 0.48797033727169037\n",
      "Loss 1 0.41946732997894287\n",
      "Loss 2 0.4001956805586815\n",
      "Loss 3 0.3828123062849045\n",
      "Loss 4 0.36625541746616364\n",
      "Loss 5 0.35026172548532486\n",
      "Loss 6 0.33474549651145935\n",
      "Loss 7 0.3199915662407875\n",
      "Loss 8 0.30610989034175873\n",
      "Loss 9 0.293178953230381\n",
      "Loss 10 0.28137246146798134\n",
      "Loss 11 0.27068719640374184\n",
      "Loss 12 0.2608467675745487\n",
      "Loss 13 0.251900814473629\n",
      "Loss 14 0.24379971250891685\n",
      "Loss 15 0.23633352667093277\n",
      "Loss 16 0.22943371534347534\n",
      "Loss 17 0.22326109558343887\n",
      "Loss 18 0.21735837683081627\n",
      "Loss 19 0.2123998925089836\n",
      "Loss 20 0.20670339092612267\n",
      "Loss 21 0.20855922624468803\n",
      "Loss 22 0.2260448820888996\n",
      "Loss 23 0.24780678376555443\n",
      "Loss 24 0.19470934197306633\n",
      "test error Net 18.90% \n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    model = Net()\n",
    "    train_model(model, train_input, train_target, mini_batch_size=200)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size=200)\n",
    "    print('test error Net {:0.2f}% '.format((100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the number of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self,hidden):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 0.1249152161180973\n",
      "Loss 1 0.14082168135792017\n",
      "Loss 2 0.12776660732924938\n",
      "Loss 3 0.1253474112600088\n",
      "Loss 4 0.12718419451266527\n",
      "Loss 5 0.12109488155692816\n",
      "Loss 6 0.11611191555857658\n",
      "Loss 7 0.11925851739943027\n",
      "Loss 8 0.12263297755271196\n",
      "Loss 9 0.11591476760804653\n",
      "Loss 10 0.11490097176283598\n",
      "Loss 11 0.11930596921592951\n",
      "Loss 12 0.11484304629266262\n",
      "Loss 13 0.11384119000285864\n",
      "Loss 14 0.11704486701637506\n",
      "Loss 15 0.11057753209024668\n",
      "Loss 16 0.10745494347065687\n",
      "Loss 17 0.10853250417858362\n",
      "Loss 18 0.10845715831965208\n",
      "Loss 19 0.10595536045730114\n",
      "Loss 20 0.10480410512536764\n",
      "Loss 21 0.10682323016226292\n",
      "Loss 22 0.1059929532930255\n",
      "Loss 23 0.10443603713065386\n",
      "Loss 24 0.10716575663536787\n",
      "Number hidden units: 10\n",
      "test error Net1 for 7.00% \n",
      "Loss 0 0.1039607860147953\n",
      "Loss 1 0.09965227637439966\n",
      "Loss 2 0.09778857324272394\n",
      "Loss 3 0.09790495689958334\n",
      "Loss 4 0.0998331243172288\n",
      "Loss 5 0.10394923854619265\n",
      "Loss 6 0.10192398447543383\n",
      "Loss 7 0.09590509254485369\n",
      "Loss 8 0.0951544139534235\n",
      "Loss 9 0.09526616707444191\n",
      "Loss 10 0.09528763499110937\n",
      "Loss 11 0.09424998890608549\n",
      "Loss 12 0.09295010846108198\n",
      "Loss 13 0.0915722344070673\n",
      "Loss 14 0.09052062593400478\n",
      "Loss 15 0.09173045260831714\n",
      "Loss 16 0.09176329709589481\n",
      "Loss 17 0.089276063721627\n",
      "Loss 18 0.08882302045822144\n",
      "Loss 19 0.0890001468360424\n",
      "Loss 20 0.0881507066078484\n",
      "Loss 21 0.08870365424081683\n",
      "Loss 22 0.09067744947969913\n",
      "Loss 23 0.08742241747677326\n",
      "Loss 24 0.0842153332196176\n",
      "Number hidden units: 50\n",
      "test error Net1 for 6.20% \n",
      "Loss 0 0.08559751650318503\n",
      "Loss 1 0.08655021106824279\n",
      "Loss 2 0.08281500497832894\n",
      "Loss 3 0.08120737411081791\n",
      "Loss 4 0.08235603850334883\n",
      "Loss 5 0.08390015503391623\n",
      "Loss 6 0.08230747980996966\n",
      "Loss 7 0.08121587289497256\n",
      "Loss 8 0.08183368621394038\n",
      "Loss 9 0.08010148629546165\n",
      "Loss 10 0.07739091478288174\n",
      "Loss 11 0.07813138561323285\n",
      "Loss 12 0.08128270227462053\n",
      "Loss 13 0.0802005436271429\n",
      "Loss 14 0.07735434314236045\n",
      "Loss 15 0.07699863146990538\n",
      "Loss 16 0.07726437225937843\n",
      "Loss 17 0.07668366190046072\n",
      "Loss 18 0.07578643690794706\n",
      "Loss 19 0.07427858980372548\n",
      "Loss 20 0.07410110393539071\n",
      "Loss 21 0.07649437757208943\n",
      "Loss 22 0.0776873892173171\n",
      "Loss 23 0.07480554841458797\n",
      "Loss 24 0.07358817663043737\n",
      "Number hidden units: 200\n",
      "test error Net1 for 5.60% \n",
      "Loss 0 0.07335896091535687\n",
      "Loss 1 0.07197748590260744\n",
      "Loss 2 0.07051042141392827\n",
      "Loss 3 0.07055734004825354\n",
      "Loss 4 0.07121205981820822\n",
      "Loss 5 0.07067175814881921\n",
      "Loss 6 0.07113976636901498\n",
      "Loss 7 0.07162786135450006\n",
      "Loss 8 0.06936852494254708\n",
      "Loss 9 0.06818048981949687\n",
      "Loss 10 0.06801882572472095\n",
      "Loss 11 0.06794974021613598\n",
      "Loss 12 0.06922755902633071\n",
      "Loss 13 0.07155499421060085\n",
      "Loss 14 0.07020994694903493\n",
      "Loss 15 0.06955438200384378\n",
      "Loss 16 0.07052804669365287\n",
      "Loss 17 0.06627971353009343\n",
      "Loss 18 0.0638677142560482\n",
      "Loss 19 0.0646637030877173\n",
      "Loss 20 0.06522342516109347\n",
      "Loss 21 0.06423262972384691\n",
      "Loss 22 0.06322778109461069\n",
      "Loss 23 0.06268710782751441\n",
      "Loss 24 0.06267222482711077\n",
      "Number hidden units: 500\n",
      "test error Net1 for 5.30% \n",
      "Loss 0 0.06259966921061277\n",
      "Loss 1 0.06288700643926859\n",
      "Loss 2 0.06414750963449478\n",
      "Loss 3 0.06333861080929637\n",
      "Loss 4 0.06034400872886181\n",
      "Loss 5 0.059477124363183975\n",
      "Loss 6 0.06133261043578386\n",
      "Loss 7 0.062410686165094376\n",
      "Loss 8 0.06173666752874851\n",
      "Loss 9 0.06033210502937436\n",
      "Loss 10 0.058273084461688995\n",
      "Loss 11 0.057170648127794266\n",
      "Loss 12 0.0603004707954824\n",
      "Loss 13 0.0634837900288403\n",
      "Loss 14 0.05940582416951656\n",
      "Loss 15 0.05653802305459976\n",
      "Loss 16 0.05609795078635216\n",
      "Loss 17 0.05686653917655349\n",
      "Loss 18 0.057522017508745193\n",
      "Loss 19 0.056917141657322645\n",
      "Loss 20 0.05617400258779526\n",
      "Loss 21 0.055152331944555044\n",
      "Loss 22 0.054621412884444\n",
      "Loss 23 0.05709205335006118\n",
      "Loss 24 0.05922475503757596\n",
      "Number hidden units: 1000\n",
      "test error Net1 for 5.80% \n"
     ]
    }
   ],
   "source": [
    "for n_hidden in [10,50,200,500,1000]:\n",
    "    modele = Net1(n_hidden)\n",
    "    train_model(model, train_input, train_target, mini_batch_size=100)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size=100)\n",
    "    print('Number hidden units:',n_hidden)\n",
    "    print('test error Net1 for {:0.2f}% '.format((100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three convolutionnal layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self,nb_hidden):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(9 * 64, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(-1, 9 * 64)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 0.5040318518877029\n",
      "Loss 1 0.45988039672374725\n",
      "Loss 2 0.4415154233574867\n",
      "Loss 3 0.4312039315700531\n",
      "Loss 4 0.4227749705314636\n",
      "Loss 5 0.4145638644695282\n",
      "Loss 6 0.40598976612091064\n",
      "Loss 7 0.3967903032898903\n",
      "Loss 8 0.38687441498041153\n",
      "Loss 9 0.37621260434389114\n",
      "Loss 10 0.3646737188100815\n",
      "Loss 11 0.35241807252168655\n",
      "Loss 12 0.33976249396800995\n",
      "Loss 13 0.32705460488796234\n",
      "Loss 14 0.3146429471671581\n",
      "Loss 15 0.3029722683131695\n",
      "Loss 16 0.29223787412047386\n",
      "Loss 17 0.2826753333210945\n",
      "Loss 18 0.2742007002234459\n",
      "Loss 19 0.2664363197982311\n",
      "Loss 20 0.2591361738741398\n",
      "Loss 21 0.2526171989738941\n",
      "Loss 22 0.24659521505236626\n",
      "Loss 23 0.24265699461102486\n",
      "Loss 24 0.2509826421737671\n",
      "test error Net 27.50% \n",
      "Loss 0 0.5176264718174934\n",
      "Loss 1 0.47917240858078003\n",
      "Loss 2 0.4574388861656189\n",
      "Loss 3 0.4443845748901367\n",
      "Loss 4 0.43628543615341187\n",
      "Loss 5 0.43045322597026825\n",
      "Loss 6 0.42533665895462036\n",
      "Loss 7 0.42017336189746857\n",
      "Loss 8 0.41460835188627243\n",
      "Loss 9 0.4085366502404213\n",
      "Loss 10 0.40184085816144943\n",
      "Loss 11 0.39446771889925003\n",
      "Loss 12 0.38640105724334717\n",
      "Loss 13 0.37756940722465515\n",
      "Loss 14 0.36793188005685806\n",
      "Loss 15 0.3575545400381088\n",
      "Loss 16 0.3466983884572983\n",
      "Loss 17 0.3356681913137436\n",
      "Loss 18 0.3247021585702896\n",
      "Loss 19 0.31408553197979927\n",
      "Loss 20 0.3040504679083824\n",
      "Loss 21 0.2947281673550606\n",
      "Loss 22 0.2862763851881027\n",
      "Loss 23 0.27877848222851753\n",
      "Loss 24 0.27205921337008476\n",
      "test error Net 29.30% \n",
      "Loss 0 0.4888491928577423\n",
      "Loss 1 0.4615745097398758\n",
      "Loss 2 0.446513295173645\n",
      "Loss 3 0.43738190084695816\n",
      "Loss 4 0.43047671020030975\n",
      "Loss 5 0.4240102022886276\n",
      "Loss 6 0.417289674282074\n",
      "Loss 7 0.4101528823375702\n",
      "Loss 8 0.402536503970623\n",
      "Loss 9 0.39431459456682205\n",
      "Loss 10 0.3854564428329468\n",
      "Loss 11 0.3759281411767006\n",
      "Loss 12 0.36570341885089874\n",
      "Loss 13 0.3548714742064476\n",
      "Loss 14 0.3436932787299156\n",
      "Loss 15 0.33245954662561417\n",
      "Loss 16 0.3213815800845623\n",
      "Loss 17 0.31078584492206573\n",
      "Loss 18 0.30101045221090317\n",
      "Loss 19 0.29216525703668594\n",
      "Loss 20 0.28418058902025223\n",
      "Loss 21 0.276981420814991\n",
      "Loss 22 0.27043136581778526\n",
      "Loss 23 0.26441000774502754\n",
      "Loss 24 0.2587616294622421\n",
      "test error Net 33.70% \n",
      "Loss 0 0.4892156198620796\n",
      "Loss 1 0.45583605021238327\n",
      "Loss 2 0.44188351929187775\n",
      "Loss 3 0.43473120778799057\n",
      "Loss 4 0.42941057682037354\n",
      "Loss 5 0.42431581020355225\n",
      "Loss 6 0.4190165027976036\n",
      "Loss 7 0.4133092239499092\n",
      "Loss 8 0.40706561505794525\n",
      "Loss 9 0.40014326572418213\n",
      "Loss 10 0.39246904850006104\n",
      "Loss 11 0.38395972549915314\n",
      "Loss 12 0.37453456223011017\n",
      "Loss 13 0.36415524035692215\n",
      "Loss 14 0.352987639605999\n",
      "Loss 15 0.34133773297071457\n",
      "Loss 16 0.32933880388736725\n",
      "Loss 17 0.31728167086839676\n",
      "Loss 18 0.3054732270538807\n",
      "Loss 19 0.2941438890993595\n",
      "Loss 20 0.28356362506747246\n",
      "Loss 21 0.27385809272527695\n",
      "Loss 22 0.264961339533329\n",
      "Loss 23 0.2569001652300358\n",
      "Loss 24 0.2494688518345356\n",
      "test error Net 28.80% \n",
      "Loss 0 0.5021035447716713\n",
      "Loss 1 0.4748605266213417\n",
      "Loss 2 0.4568974897265434\n",
      "Loss 3 0.44511382281780243\n",
      "Loss 4 0.43735072016716003\n",
      "Loss 5 0.43152762949466705\n",
      "Loss 6 0.4263571724295616\n",
      "Loss 7 0.4211812764406204\n",
      "Loss 8 0.4158761501312256\n",
      "Loss 9 0.4103297144174576\n",
      "Loss 10 0.4044453129172325\n",
      "Loss 11 0.39818617701530457\n",
      "Loss 12 0.39147500693798065\n",
      "Loss 13 0.3842843398451805\n",
      "Loss 14 0.37656962871551514\n",
      "Loss 15 0.36830706894397736\n",
      "Loss 16 0.3594941347837448\n",
      "Loss 17 0.3501434400677681\n",
      "Loss 18 0.34038347750902176\n",
      "Loss 19 0.3303464278578758\n",
      "Loss 20 0.32019128650426865\n",
      "Loss 21 0.3101399652659893\n",
      "Loss 22 0.30039041489362717\n",
      "Loss 23 0.2911769710481167\n",
      "Loss 24 0.28259456157684326\n",
      "test error Net 32.00% \n",
      "Loss 0 0.5149290412664413\n",
      "Loss 1 0.44987254589796066\n",
      "Loss 2 0.43323757499456406\n",
      "Loss 3 0.423094779253006\n",
      "Loss 4 0.41298186033964157\n",
      "Loss 5 0.4026399552822113\n",
      "Loss 6 0.3918219730257988\n",
      "Loss 7 0.3801868185400963\n",
      "Loss 8 0.3679492920637131\n",
      "Loss 9 0.3555230423808098\n",
      "Loss 10 0.3433035612106323\n",
      "Loss 11 0.3314887061715126\n",
      "Loss 12 0.32009950280189514\n",
      "Loss 13 0.30927881598472595\n",
      "Loss 14 0.2992102764546871\n",
      "Loss 15 0.2898031212389469\n",
      "Loss 16 0.2812405042350292\n",
      "Loss 17 0.2733006030321121\n",
      "Loss 18 0.26675082743167877\n",
      "Loss 19 0.289191797375679\n",
      "Loss 20 0.3087824769318104\n",
      "Loss 21 0.2518956586718559\n",
      "Loss 22 0.24521132558584213\n",
      "Loss 23 0.2394239753484726\n",
      "Loss 24 0.23551346734166145\n",
      "test error Net 33.40% \n",
      "Loss 0 0.5043183639645576\n",
      "Loss 1 0.46701887249946594\n",
      "Loss 2 0.44969049096107483\n",
      "Loss 3 0.4401166960597038\n",
      "Loss 4 0.4331067278981209\n",
      "Loss 5 0.4264220520853996\n",
      "Loss 6 0.4193977788090706\n",
      "Loss 7 0.41182973980903625\n",
      "Loss 8 0.4036203920841217\n",
      "Loss 9 0.3947507217526436\n",
      "Loss 10 0.38510292023420334\n",
      "Loss 11 0.3746194541454315\n",
      "Loss 12 0.3633311316370964\n",
      "Loss 13 0.35155579447746277\n",
      "Loss 14 0.3396304175257683\n",
      "Loss 15 0.3280368074774742\n",
      "Loss 16 0.3169608488678932\n",
      "Loss 17 0.3065396286547184\n",
      "Loss 18 0.29685284942388535\n",
      "Loss 19 0.2879627123475075\n",
      "Loss 20 0.2799205929040909\n",
      "Loss 21 0.2725774981081486\n",
      "Loss 22 0.26584719493985176\n",
      "Loss 23 0.2595481090247631\n",
      "Loss 24 0.25356265902519226\n",
      "test error Net 29.70% \n",
      "Loss 0 0.4698590189218521\n",
      "Loss 1 0.45345303416252136\n",
      "Loss 2 0.44519731402397156\n",
      "Loss 3 0.44017650932073593\n",
      "Loss 4 0.4361245110630989\n",
      "Loss 5 0.4321597293019295\n",
      "Loss 6 0.4279934763908386\n",
      "Loss 7 0.42354893684387207\n",
      "Loss 8 0.41882240772247314\n",
      "Loss 9 0.41392136365175247\n",
      "Loss 10 0.4086669981479645\n",
      "Loss 11 0.403000570833683\n",
      "Loss 12 0.39683273434638977\n",
      "Loss 13 0.3901037350296974\n",
      "Loss 14 0.38287453353405\n",
      "Loss 15 0.37517356872558594\n",
      "Loss 16 0.3670341968536377\n",
      "Loss 17 0.3584637939929962\n",
      "Loss 18 0.34950943291187286\n",
      "Loss 19 0.34033720195293427\n",
      "Loss 20 0.3311288133263588\n",
      "Loss 21 0.3220396563410759\n",
      "Loss 22 0.31322334334254265\n",
      "Loss 23 0.30470194295048714\n",
      "Loss 24 0.29648809507489204\n",
      "test error Net 39.00% \n",
      "Loss 0 0.48497194051742554\n",
      "Loss 1 0.4488033428788185\n",
      "Loss 2 0.4326828792691231\n",
      "Loss 3 0.42298920452594757\n",
      "Loss 4 0.4140826463699341\n",
      "Loss 5 0.40464136749505997\n",
      "Loss 6 0.39452292770147324\n",
      "Loss 7 0.3835856467485428\n",
      "Loss 8 0.37210557609796524\n",
      "Loss 9 0.36037059873342514\n",
      "Loss 10 0.34862179309129715\n",
      "Loss 11 0.3372819125652313\n",
      "Loss 12 0.3266337141394615\n",
      "Loss 13 0.3169146627187729\n",
      "Loss 14 0.30815866217017174\n",
      "Loss 15 0.30031925439834595\n",
      "Loss 16 0.29323434084653854\n",
      "Loss 17 0.28670784085989\n",
      "Loss 18 0.28062397986650467\n",
      "Loss 19 0.27488474920392036\n",
      "Loss 20 0.26943041011691093\n",
      "Loss 21 0.2644853815436363\n",
      "Loss 22 0.25924912840127945\n",
      "Loss 23 0.2559944950044155\n",
      "Loss 24 0.2549743503332138\n",
      "test error Net 41.50% \n",
      "Loss 0 0.49742191284894943\n",
      "Loss 1 0.4574591591954231\n",
      "Loss 2 0.43951334059238434\n",
      "Loss 3 0.4280300438404083\n",
      "Loss 4 0.417293019592762\n",
      "Loss 5 0.4060243219137192\n",
      "Loss 6 0.3941384181380272\n",
      "Loss 7 0.38141249865293503\n",
      "Loss 8 0.36810681223869324\n",
      "Loss 9 0.35468898713588715\n",
      "Loss 10 0.34140151739120483\n",
      "Loss 11 0.3285154476761818\n",
      "Loss 12 0.31635040044784546\n",
      "Loss 13 0.3049519993364811\n",
      "Loss 14 0.2942880876362324\n",
      "Loss 15 0.2849241532385349\n",
      "Loss 16 0.2768227122724056\n",
      "Loss 17 0.32606402039527893\n",
      "Loss 18 0.2819009907543659\n",
      "Loss 19 0.25844091176986694\n",
      "Loss 20 0.25080234184861183\n",
      "Loss 21 0.24400438368320465\n",
      "Loss 22 0.23795654997229576\n",
      "Loss 23 0.2323647253215313\n",
      "Loss 24 0.22935158759355545\n",
      "test error Net 27.80% \n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    model = Net2(200)\n",
    "    train_model(model, train_input, train_target, mini_batch_size=200)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, test_target, mini_batch_size=200)\n",
    "    print('test error Net {:0.2f}% '.format((100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
